#ğŸ¥ Insurance Premium Prediction â€” Productionâ€‘Grade Hybrid Regression System

> **A dataâ€‘science case study focused on accuracy, robustness, and error reduction in realâ€‘world pricing models.**

---

## ğŸ“Œ Project Overview

Accurately predicting insurance premiums is a **highâ€‘stakes regression problem** where traditional metrics (like RÂ²) alone are insufficient. Even models with strong average performance can fail catastrophically on specific customer segments, leading to severe underâ€‘ or overâ€‘pricing.

This project goes beyond baseline modeling by:

* Performing **deep error analysis**
* Identifying and mitigating **extreme relative errors**
* Designing a **hybrid residual learning architecture** to improve stability and reliability

The final system achieves **high predictive power while nearly eliminating extreme prediction failures**, making it suitable for realâ€‘world deployment scenarios.

---

## ğŸ¯ Problem Statement

Given customer demographic, lifestyle, and financial attributes, predict the **annual insurance premium amount** as accurately and robustly as possible.

Key challenges addressed:

* Highly skewed target variable
* Sparse and categorical feature interactions
* Disproportionate impact of errors on lowâ€‘premium customers
* Large percentage errors despite good global metrics

---

## ğŸ§  Modeling Philosophy

Rather than optimizing a single metric, this project emphasizes:

* **Error distribution analysis** over average accuracy
* **Tailâ€‘risk mitigation** (reducing extreme relative errors)
* **Interpretability + nonâ€‘linear learning** through hybrid modeling

> *A model that performs well on average but fails badly for 30% of cases is not productionâ€‘ready.*

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **Pandas, NumPy** â€” data processing
* **scikitâ€‘learn** â€” preprocessing, Linear Regression
* **XGBoost** â€” residual learning
* **Matplotlib, Seaborn** â€” visualization

---

## ğŸ“Š Dataset & Features

### Target

* `annual_premium_amount` â†’ logâ€‘transformed for stability

### Feature Categories

* Demographics: age, gender, marital status, region
* Financials: income (continuous & categorical)
* Lifestyle: BMI category, smoking status
* Risk indicators: medical history, insurance plan

Categorical variables were oneâ€‘hot encoded. Numeric features were scaled where appropriate.

---

## ğŸ”„ Project Workflow

### 1ï¸âƒ£ Exploratory Data Analysis

* Identified heavy rightâ€‘skew in premium amounts
* Applied log transformation to stabilize variance

### 2ï¸âƒ£ Baseline Linear Regression

* Achieved strong global performance (RÂ² â‰ˆ 0.94)
* **But** error analysis revealed:

  * ~33% of test samples had >10% relative error
  * Extreme errors clustered around lowâ€‘income and sparse feature combinations

### 3ï¸âƒ£ Interaction Feature Engineering

* Created domainâ€‘informed interaction terms
* Improved structural expressiveness
* Introduced multicollinearity and variance tradeâ€‘offs

### 4ï¸âƒ£ Residual Learning Architecture (Core Contribution)

A **twoâ€‘stage hybrid model** was implemented:

1. **Linear Regression**

   * Captures global linear trends
   * Maintains interpretability

2. **XGBoost Regressor (Residual Model)**

   * Trained on prediction residuals
   * Learns nonâ€‘linear patterns missed by linear model

Final prediction:

```
Final Prediction = Linear Prediction + XGBoost Residual Prediction
```

---

## âœ… Final Results

| Metric                    | Baseline Linear Model | Hybrid Model |
| ------------------------- | --------------------- | ------------ |
| RÂ²                        | ~0.94                 | **0.926**    |
| RMSE (log scale)          | ~0.20                 | **0.157** â†“  |
| Extreme Error Rate (>10%) | **~33.7%**            | **0.8%** ğŸ”¥  |

### Key Takeaway

> The hybrid model **nearly eliminates catastrophic prediction failures** while retaining high explanatory power.

---

## ğŸ“‰ Error Analysis & Visualization Highlights

* Residual plots revealed heteroscedasticity in baseline models
* Feature distribution analysis showed **no systematic concentration of extreme errors** after hybrid modeling
* KDE plots were replaced with **rug plots** where data scarcity made density estimation statistically invalid

This ensured **honest and interpretable visualizations**.

---

## ğŸ§ª Why This Matters in Production

In real insurance systems:

* A small number of bad predictions can cause large financial or regulatory impact
* Tailâ€‘risk matters more than marginal RÂ² improvements

This project demonstrates:

* Metricâ€‘driven debugging
* Robust pipeline design
* Practical ML engineering decisions

---

## ğŸ“ˆ Possible Extensions

* Quantile regression for uncertainty estimation
* SHAP analysis for residual model interpretability
* Deployment via Streamlit or FastAPI
* Monitoring drift in residual distributions

---

## ğŸ Conclusion

This project demonstrates how **errorâ€‘aware modeling and hybrid architectures** can transform a strong baseline into a **productionâ€‘ready regression system**.

It reflects realâ€‘world ML work: diagnosing failures, iterating intelligently, and prioritizing robustness over vanity metrics.

---

## ğŸ‘¤ Author

**Shubham Jha**
Aspiring Data Scientist | Machine Learning Enthusiast

---

â­ *If you find this project insightful, feel free to star the repository or reach out for discussion.*
